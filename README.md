# Portfolio Analista de Dados

<h1>Análise Exploratória de Dados em Python</h1>

A Loggi é uma empresa que usa a tecnologia para inovar os envios de pacotes para todo o Brasil com praticidade e no tempo que você precisa. Por aqui, a gente acredita que uma logística de qualidade é um direito de todas as pessoas.

Nesse Projeto Análise Exploratória de Dados, foi feito a exploração dados de logística, para entregar de pedidos como local de início e destino do produto, com o intuito de conhecer os hubs distribuídos no Distrito Federal (DF), bem como a proporção e os desafios que a Loggi enfrenta diante de todo esse cenário logístico de entregas.

>**Dados**: coletados da empressa Loggi.

>**Processamento**: Notebook Python via Google Colab

>**Deploy**: [Github](https://github.com/petwillian/Portfolio-Analista-de-Dados/blob/main/Projeto%20An%C3%A1lise%20Explorat%C3%B3ria%20de%20Dados%20em%20Python/Projeto.ipynb).

>**Ferramentas**: Python (Google Colab) e Geopandas .

![image](https://github.com/petwillian/Portfolio-Analista-de-Dados/blob/main/Projeto%20An%C3%A1lise%20Explorat%C3%B3ria%20de%20Dados%20em%20Python/Entregas%20no%20Distrito%20Federal.png)

**Insights**

As entregas parecem estar corretamente alocadas em seus respectivos Hubs. A distância média percorrida para a realização de uma entrega considerando os três hubs é de 7.1 km, e a distância mediana é de 5.53km.

No entanto, como os Hubs das regiões 0 e 2 fazem entregas em locais mais distantes do centro, isso pode gerar um tempo e preço de entrega maiores.

![Quantidade de entregas por cidade](https://github.com/petwillian/Portfolio-Analista-de-Dados/blob/main/Projeto%20An%C3%A1lise%20Explorat%C3%B3ria%20de%20Dados%20em%20Python/Quantidade%20de%20entregas%20por%20cidade.png)

**Insights**

Mais uma vez vemos que há uma concentração de entregas em Brasília, onde está localizado o Hub df-1. Isso justifica, de certa forma, uma concentração maior dos veículos de entrega nesse Hub. Entretanto, há também uma dispersão de entregas em outras regiões. De novo, se olharmos o mapa, vemos que o Hub de Sobradinho possui entregas distantes do centro de distribuição, ao mesmo tempo que possui menos veículos de entrega quando comparado com outros Hubs.

# Dashboard de dados

Dashboard O produto final desta análise se encontre em um dashboard do Google Data Studio. O dashboard de dados contem os seguintes elementos:

Key performance indicator (KPIs):

Casos e mortes nas 24 horas; Média móvel (7 dias) de casos e mortes; Tendência de casos e mortes; Proporção de vacinados com 1ª, 2ª e 3º dose. Exploratory Data Analysis (EDA) interativa:

Distribuição do números de casos e mortes ao longo do tempo; Distribuição da média móvel (7 dias) do números de casos e mortes ao longo do tempo; Distribuição geográfica dos casos por estado por dia.

>**Dados**: coletados do Github.

>**Processamento**: Notebook Python via Google Colab

>**Deploy**: [lookerstudio](https://lookerstudio.google.com/reporting/0b29e22d-8d1c-4076-bdee-c08921f08c18/page/90rHE)

>**Ferramentas**: Python (Google Colab) e lookerstudio .

![Covid_19](https://github.com/petwillian/Portfolio-Analista-de-Dados/blob/main/Dashboard%20de%20dados/Covid_19.jpg)

# Exploração e análise de dados de crédito com SQL

Essas foram algumas das análises extraídas do conjunto de dados, fornecendo insights sobre o perfil dos clientes e seus comportamentos de transação. 

>**Dados**: coletados do [Credito.csv](https://github.com/petwillian/Portfolio-Analista-de-Dados/blob/main/Projeto%20SQL/credito.csv)

>**Processamento**: Notebook Python via Google Colab

>**Deploy**: [Github](https://github.com/petwillian/Portfolio-Analista-de-Dados/tree/main/Projeto%20SQL)

>**Ferramentas**: AWS S3, AWS Athena, Python (Google Colab).


![sql-credito](https://github.com/petwillian/Portfolio-Analista-de-Dados/blob/main/Projeto%20SQL/sql-credito.png)

# Projeto Pipeline de dados do Telegram

O objetivo deste projecto foi implementar um pipeline dades para analisar mensagens em um grupo do telegram. O pipeline foi implementado utilizado serviços da Amzon Web Services (AWS).

>**Dados**: coletados do [Credito.csv](https://github.com/petwillian/Portfolio-Analista-de-Dados/blob/main/Projeto%20SQL/credito.csv)

>**Processamento**: Notebook Python via Google Colab

>**Deploy**: [Github](https://github.com/petwillian/Portfolio-Analista-de-Dados/tree/main/Projeto%20SQL)

>**Ferramentas**: AWS S3, AWS Athena, Python (Google Colab).

![Profissao Analista de dados M42 Material de apoio arch (1)](https://github.com/petwillian/Portfolio-Analista-de-Dados/blob/main/Projeto%20Pipeline%20de%20dados%20do%20Telegram/%20Arch.png)



